{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/boston.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input features in order:\n",
    "1) CRIM: per capita crime rate by town\n",
    "2) ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3) INDUS: proportion of non-retail business acres per town\n",
    "4) CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "5) NOX: nitric oxides concentration (parts per 10 million) [parts/10M]\n",
    "6) RM: average number of rooms per dwelling\n",
    "7) AGE: proportion of owner-occupied units built prior to 1940\n",
    "8) DIS: weighted distances to five Boston employment centres\n",
    "9) RAD: index of accessibility to radial highways\n",
    "10) TAX: full-value property-tax rate per $10,000 [$/10k]\n",
    "11) PTRATIO: pupil-teacher ratio by town\n",
    "12) B: The result of the equation B=1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "13) LSTAT: % lower status of the population\n",
    "\n",
    "Output variable:\n",
    "1) MEDV: Median value of owner-occupied homes in $1000's [k$]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get familiar with the data, check the shape, the first 5 rows, df.describe(), df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did you notice anything weird in df.describe()? If not, check out MEDV's max value, the min value and the mean value. \n",
    "# What's going on? Is this a problem? How can you fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: there is a function in pandas made just for this purpose\n",
    "# HINT 2: https://google.gprivate.com/search.php?search?q=pandas+remove+duplicates\n",
    "df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the XGBoost regression model. XGBoost stands for: eXtreme Gradient Boosting. \n",
    "# This is a very popular algorithm, used in machine learning competitions and in the industry. \n",
    "# We will use it for regression, but it can also be used for classification.\n",
    "\n",
    "model = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let y be the target column, and X be the rest of the df\n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets with the function train_test_split from sklearn. Use test_size=0.2 and random_state=42\n",
    "# We use train_test_split to split the data into train and test sets. We will use the train set to train the model, and the test set to evaluate the model.\n",
    "# The reason we need a test set is to be able to evaluate the model. If we train the model on the whole dataset, \n",
    "# it will learn the dataset perfectly, but we will not know how it performs on unseen data. \n",
    "\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the training set (X_train, y_train) to train the model by calling the .fit() method\n",
    "model.fit(?, ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict the target values for the test set (X_test)\n",
    "preds = model.predict(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mean squared error for the predictions (a value to see the value of the predictions, lower is better)\n",
    "# find the error between the y_test and the preds\n",
    "mse = mean_squared_error(?, ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the mse to see how much, on average, your model is off (squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are some of the hyperparameters you can tune for XGBoost. \n",
    "# A hyperparameter is a parameter that is not learned by the model, but is set by the user.\n",
    "# The parameters that are learned by the model are called model parameters.\n",
    "# The model starts off with some default values for the hyperparameters, but you can change them to get potentially better results.\n",
    "# This process is called hyperparameter tuning.\n",
    "\n",
    "# If you want, you can adjust the hyperparameters and see if you can get a better result. You can also add more hyperparameters to the dictionary.\n",
    "# List of hyperparameters: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "params = {\n",
    "    \"learning_rate\": [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n",
    "    \"max_depth\": [3, 4, 5, 6, 8, 10, 12, 15],\n",
    "    \"min_child_weight\": [1, 3, 5, 7],\n",
    "    \"gamma\": [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    \"colsample_bytree\": [0.3, 0.4, 0.5, 0.7],\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500, 900, 1100, 1500],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RandomizedSearchCV to find the best hyperparameters for the model. There are other ways to do this, but random search will work for this purpose.\n",
    "# Random search is a method for hyperparameter tuning that will try a given number of random combinations of hyperparameters.\n",
    "# Use the training set (X_train, y_train) to instantiate the random search by calling the .fit() method with the test set\n",
    "# HINT: n_iter is the number of iterations to run the random search, if this number is too high, it will take a long time to run, \n",
    "# but if it's too low, it will not find the best hyperparameters. You should try to find a happy medium.\n",
    "\n",
    "# First, create a new, similar model, but with the default hyperparameters. Do not fit this model with the training set.\n",
    "model2 = \n",
    "\n",
    "random_search = RandomizedSearchCV(?, param_distributions=params, n_iter=?, scoring=\"neg_mean_squared_error\", n_jobs=-1, cv=5)\n",
    "\n",
    "# Fit the model with x and y train sets\n",
    "random_search.fit(?, ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best model/estimator from the random search\n",
    "model_new = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new predictions with the new model\n",
    "preds = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the new mean square error\n",
    "mse_new = ?\n",
    "\n",
    "mse_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"relation between better error on the new model and the old error: {(mse_new / mse)}\")\n",
    "\n",
    "# If the new model did not perform better, this means that the default hyperparameters were better, but it is highly likely that even better ones exist.\n",
    "# You can try to run the random search again, but with more iterations, or you can try to use GridSearchCV instead of RandomizedSearchCV ot test _every_ combination of hyperparameters.\n",
    "# You can also edit the hyperparameters in the dictionary to see if you can get better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
